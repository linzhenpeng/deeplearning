# 损失函数

<https://blog.csdn.net/chkay399/article/details/81878157>

<https://www.jianshu.com/p/b715888f079b>

<https://github.com/groverpr/Machine-Learning/blob/master/notebooks/05_Loss_Functions.ipynb>

<http://www.mamicode.com/info-detail-2346633.html>

损失函数（loss function）是用来估量你模型的预测值f(x)与真实值Y的不一致程度，它是一个非负实值函数,通常使用L(Y, f(x))来表示，损失函数越小，模型的鲁棒性就越好。损失函数是**经验风险函数**的核心部分，也是**结构风险函数**重要组成部分。模型的结构风险函数包括了经验风险项和正则项(L0、L1（Lasso）、L2（Ridge）)，通常可以表示成如下式子：

![](H:\git\deeplearning\images\损失函数\20180623145555155265.png)

其中，前面的均值函数表示的是经验风险函数，L代表的是损失函数，后面的Φ是正则化项（regularizer）或者叫惩罚项（penalty term），它可以是L1，也可以是L2，或者其他的正则函数。整个式子表示的意思是**找到使目标函数最小时的θ值**。



在机器学习中，所有的机器学习算法都或多或少的依赖于对目标函数最大化或者最小化的过程，我们常常把最小化的函数称为损失函数，它主要用于衡量机器学习模型的预测能力。在寻找最小值的过程中，我们最常用的方法是梯度下降法。

虽然损失函数可以让我们看到模型的优劣，并且为我们提供了优化的方向，但是我们必须知道没有任何一种损失函数适用于所有的模型。损失函数的选取依赖于参数的数量、异常值、机器学习算法、梯度下降的效率、导数求取的难易和预测的置信度等若干方面。这篇文章将介绍各种不同的损失函数，并帮助我们理解每种损失函数的优劣和适用范围。

由于机器学习的任务不同，损失函数一般分为分类和回归两类，回归会预测给出一个数值结果而分类则会给出一个标签。

![](H:\git\deeplearning\images\损失函数\2509688-9afce072a05bf6b8.png)

## **回归损失函数**

### **MSE均方误差、平方损失——L2损失：**

均方误差（MSE）是回归损失函数中最常用的误差，它是预测值与目标值之间差值的平方和，其公式如下所示：

![](H:\git\deeplearning\images\损失函数\2509688-ff0b77dc22a5f5da.webp)


下图是均方根误差值的曲线分布，其中最小值为预测值为目标值的位置。我们可以看到随着误差的增加损失函数增加的更为迅猛。


  ![](H:\git\deeplearning\images\损失函数\2509688-5d75f32483cdd9e0.webp)

### **MAE平均绝对误差**——L1损失函数：

平均绝对误差（MAE）是另一种常用的回归损失函数，它是目标值与预测值之差绝对值的和，表示了预测值的平均误差幅度，而不需要考虑误差的方向（注：平均偏差误差MBE则是考虑的方向的误差，是残差的和），范围是0到∞，其公式如下所示：

![](H:\git\deeplearning\images\损失函数\2509688-faed3d26964f559d.webp)



![](H:\git\deeplearning\images\损失函数\2509688-8d4972804ab1f990.webp)

**平均绝对误差和均方误差（**L1&L2**）比较：**

通常来说，利用均方差更容易求解，但平方绝对误差则对于异常值更稳健，下面让我们对这两种损失函数进行具体的分析。

无论哪一种机器学习模型，目标都是找到能使目标函数最小的点。在最小值处每一种损失函数都会得到最小值。但哪种是更好的指标呢？你可以上述笔记本地址自行运行代码，检查它们的各项指标。

让我们用具体例子看一下，下图是均方根误差和平均绝对误差的比较（其中均方根误差的目的是与平均绝对误差在量级上统一）:

![](H:\git\deeplearning\images\损失函数\2509688-6c85b5f3713e5996.webp)

左边的图中预测值与目标值很接近，误差与方差都很小，而右边的图中由于异常值的存在使得误差变得很大。

由于均方误差（MSE）在误差较大点时的损失远大于平均绝对误差（MAE），它会给异常值赋予更大的权重，模型会全力减小异常值造成的误差，从而使得模型的整体表现下降。

所以当训练数据中含有较多的异常值时，平均绝对误差（MAE）更为有效。当我们对所有观测值进行处理时，如果利用MSE进行优化则我们会得到所有观测的均值，而使用MAE则能得到所有观测的中值。与均值相比，中值对于异常值的鲁棒性更好，这就意味着平均绝对误差对于异常值有着比均方误差更好的鲁棒性。

但MAE也存在一个问题，特别是对于神经网络来说，它的梯度在极值点处会有很大的跃变，及时很小的损失值也会长生很大的误差，这很不利于学习过程。为了解决这个问题，需要在解决极值点的过程中动态减小学习率。MSE在极值点却有着良好的特性，及时在固定学习率下也能收敛。MSE的梯度随着损失函数的减小而减小，这一特性使得它在最后的训练过程中能得到更精确的结果（如下图）。


  ![](H:\git\deeplearning\images\损失函数\2509688-ce3f5dcd7140ea95.webp)



在实际训练过程中，如果异常值对于实际业务十分重要需要进行检测，MSE是更好的选择，而如果在异常值极有可能是坏点的情况下MAE则会带来更好的结果。

总结：L1损失对于异常值更鲁棒，但它的导数不连续使得寻找最优解的过程低效；L2损失对于异常值敏感，但在优化过程中更为稳定和准确。更详细的L1和L2不同比较可以[参考这篇文章](http://rishy.github.io/ml/2015/07/28/l1-vs-l2-loss/)。

但现实中还存在两种损失都很难处理的问题。例如某个任务中90%的数据都符合目标值——150，而其余的10%数据取值则在0-30之间。那么利用MAE优化的模型将会得到150的预测值而忽略的剩下的10%（倾向于中值）；而对于MSE来说由于异常值会带来很大的损失，将使得模型倾向于在0-30的方向取值。这两种结果在实际的业务场景中都是我们不希望看到的。



### Huber损失——平滑平均绝对误差

Huber损失相比于平方损失来说对于异常值不敏感，但它同样保持了可微的特性。它基于绝对误差但在误差很小的时候变成了平方误差。我们可以使用超参数δ来调节这一误差的阈值。当δ趋向于0时它就退化成了MAE，而当δ趋向于无穷时则退化为了MSE，其表达式如下，是一个连续可微的分段函数：

![](H:\git\deeplearning\images\损失函数\2509688-1306884db4a2a9cd.webp)

![](H:\git\deeplearning\images\损失函数\2509688-7bf3ce6b9538ff42.webp)

对于Huber损失来说，δ的选择十分重要，它决定了模型处理异常值的行为。当残差大于δ时使用L1损失，很小时则使用更为合适的L2损失来进行优化。

Huber损失函数克服了MAE和MSE的缺点，不仅可以保持损失函数具有连续的导数，同时可以利用MSE梯度随误差减小的特性来得到更精确的最小值，也对异常值具有更好的鲁棒性。

而Huber损失函数的良好表现得益于精心训练的超参数δ。

### **Log-Cosh损失函数**

Log-Cosh损失函数是一种比L2更为平滑的损失函数，利用双曲余弦来计算预测误差：

![](H:\git\deeplearning\images\损失函数\2509688-5a6fce788a4e153c.webp)
  ![](H:\git\deeplearning\images\损失函数\2509688-021a049e8bda30ca.webp)

它的优点在于对于很小的误差来说log(cosh(x))与（x**2）/2很相近，而对于很大的误差则与abs(x)-log2很相近。这意味着log cosh损失函数可以在拥有MSE优点的同时也不会受到异常值的太多影响。它拥有Huber的所有优点，并且在每一个点都是二次可导的。二次可导在很多机器学习模型中是十分必要的，例如使用牛顿法的XGBoost优化模型（Hessian矩阵）。

![](H:\git\deeplearning\images\损失函数\2509688-60751c2a475d8659.webp)



但是Log-cosh损失并不是完美无缺的，它还是会在很大误差的情况下梯度和hessian变成了常数。

Huber和Log-cosh损失函数的Python代码：

![](H:\git\deeplearning\images\损失函数\2509688-598d0114cca5b3e9.webp)



### **分位数损失（Quantile Loss）**

在大多数真实世界的预测问题中，我们常常希望看到我们预测结果的不确定性。通过预测出一个取值区间而不是一个个具体的取值点对于具体业务流程中的决策至关重要。

分位数损失函数在我们需要预测结果的取值区间时是一个特别有用的工具。通常情况下我们利用最小二乘回归来预测取值区间主要基于这样的假设：取值残差的方差是常数。但很多时候对于线性模型是不满足的。这时候就需要分位数损失函数和分位数回归来拯救回归模型了。它对于预测的区间十分敏感，即使在非均匀分布的残差下也能保持良好的性能。下面让我们用两个例子看看分位数损失在异方差数据下的回归表现。

![](H:\git\deeplearning\images\损失函数\2509688-47455dd9671d3d63.webp)

左：线性关系b / w X1和Y.具有恒定的残差方差。右：线性关系b / w X2和Y，但Y的方差随着X2增加。

上图是两种不同的数据分布，其中左图是残差的方差为常数的情况，而右图则是残差的方差变化的情况。我们利用正常的最小二乘对上述两种情况进行了估计，其中橙色线为建模的结果。但是我们却无法得到取值的区间范围，这时候就需要分位数损失函数来提供。





## 分类损失函数

### LogLoss对数损失函数（逻辑回归，交叉熵损失）

有些人可能觉得逻辑回归的损失函数就是平方损失，其实并不是。**平方损失函数可以通过线性回归在假设样本是高斯分布的条件下推导得到**，而逻辑回归得到的并不是平方损失。在逻辑回归的推导中，它假设样本服从**伯努利分布（0-1分布）**，然后求得满足该分布的似然函数，接着取对数求极值等等。而逻辑回归并没有求似然函数的极值，而是把极大化当做是一种思想，进而推导出它的经验风险函数为：**最小化负的似然函数（即max F(y, f(x)) —> min -F(y, f(x)))**。从损失函数的视角来看，它就成了log损失函数了。

**log损失函数的标准形式**：

![](H:\git\deeplearning\images\损失函数\20180623145556292888.png)

刚刚说到，取对数是为了方便计算极大似然估计，因为在MLE（最大似然估计）中，直接求导比较困难，所以通常都是先取对数再求导找极值点。损失函数L(Y, P(Y|X))表达的是样本X在分类Y的情况下，使概率P(Y|X)达到最大值（换言之，**就是利用已知的样本分布，找到最有可能（即最大概率）导致这种分布的参数值；或者说什么样的参数才能使我们观测到目前这组数据的概率最大**）。因为log函数是单调递增的，所以logP(Y|X)也会达到最大值，因此在前面加上负号之后，最大化P(Y|X)就等价于最小化L了。

逻辑回归的P(Y=y|x)表达式如下（为了将类别标签y统一为1和0，下面将表达式分开表示）：

![](H:\git\deeplearning\images\损失函数\20180623145557113148.png)

将它带入到上式，通过推导可以得到logistic的损失函数表达式，如下：

![](H:\git\deeplearning\images\损失函数\20180623145557894348.png)

逻辑回归最后得到的目标式子如下：

![](H:\git\deeplearning\images\损失函数\20180623145558895260.png)

上面是针对二分类而言的。这里需要解释一下：**之所以有人认为逻辑回归是平方损失，是因为在使用梯度下降来求最优解的时候，它的迭代式子与平方损失求导后的式子非常相似，从而给人一种直观上的错觉**。

这里有个PDF可以参考一下：[Lecture 6: logistic regression.pdf](https://www.cs.berkeley.edu/~russell/classes/cs194/f11/lectures/CS194%20Fall%202011%20Lecture%2006.pdf).

　　**注意：softmax使用的即为交叉熵损失函数，binary_cossentropy为二分类交叉熵损失，categorical_crossentropy为多分类交叉熵损失，当使用多分类交叉熵损失函数时，标签应该为多分类模式，即使用one-hot编码的向量。**





### hinge loss (折页损失函数、铰链损失函数)

<https://blog.csdn.net/fendegao/article/details/79968994>



### focal loss 



### **0-1损失函数**

![](H:\git\deeplearning\images\损失函数\20180623145606840064.png)